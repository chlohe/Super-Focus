{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MICCAI 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "\n",
    "# Imports for image preprocessing\n",
    "from skimage import io, exposure\n",
    "from skimage.transform import resize\n",
    "from skimage.draw import disk\n",
    "\n",
    "# For calculating SSIM\n",
    "from ssim import ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow loading of truncated images\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_stack(stack_dir_path):\n",
    "    \"\"\" Load a stack from the containing directory path. \"\"\"\n",
    "    img_paths = glob.glob(os.path.join(stack_dir_path, '*.jpg'))\n",
    "    # Create list of images tagged with their focal depths\n",
    "    imgs = [\n",
    "        (\n",
    "            int(os.path.splitext(os.path.basename(img_path))[0][1:]),\n",
    "            io.imread(img_path, as_gray=True)\n",
    "        )\n",
    "        for img_path in img_paths\n",
    "    ]\n",
    "    # Sort by focal depth\n",
    "    imgs = sorted(imgs, key=lambda x: x[0])\n",
    "    # Remove the focal depth info\n",
    "    imgs = [img for _, img in imgs]\n",
    "    return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_stack(imgs, plane_size):\n",
    "    \"\"\" Apply normalisation across the whole stack by combining planes\n",
    "        into single image and applying normalisation to that. \"\"\"\n",
    "    # Combine the images\n",
    "    combined_img = np.zeros(\n",
    "        (plane_size[0] * len(imgs), plane_size[1]))\n",
    "    for i in range(len(imgs)):\n",
    "        combined_img[i * plane_size[0]:(i + 1) * plane_size[0], 0:plane_size[1]] = imgs[i]\n",
    "    # Normalise combined image\n",
    "    combined_img = exposure.equalize_adapthist(\n",
    "        combined_img, clip_limit=0.01)\n",
    "    # Unpack the combined image into planes\n",
    "    return [combined_img[i * plane_size[0]:(i + 1) * plane_size[0], 0:plane_size[1]]\n",
    "            for i in range(len(imgs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_stack(imgs, plane_size=(400, 400), crop_proportion=0.0625):\n",
    "    \"\"\" Preprocess a stack. \"\"\"\n",
    "    # Generate mask for hiding the well outline\n",
    "    rr, cc = disk((plane_size[0]/2, plane_size[1]/2), plane_size[0]/2)\n",
    "    circle_mask = np.zeros(plane_size)\n",
    "    circle_mask[rr, cc] = 1\n",
    "    # Center-crop, resize and apply mask to all images\n",
    "    h, w = imgs[0].shape\n",
    "    h_rm, w_rm = int(h * crop_proportion), int(w * crop_proportion)\n",
    "    imgs = [img[h_rm:-h_rm, w_rm:-w_rm] for img in imgs]\n",
    "    imgs = [resize(img, plane_size) for img in imgs]\n",
    "    imgs = [img * circle_mask for img in imgs]\n",
    "    # Normalise stack\n",
    "    imgs = normalise_stack(imgs, plane_size)\n",
    "    return imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_model(model, first, second):\n",
    "    image = np.zeros((400, 400, 2))\n",
    "    image[:, :, 0] = first\n",
    "    image[:, :, 1] = second\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    image = image.cuda() if torch.cuda.is_available() else image\n",
    "    pred = model.forward(image)\n",
    "    pred = pred.squeeze().detach().cpu()\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr, cc = disk((200, 200), 200)\n",
    "circle_mask = np.zeros((400, 400))\n",
    "circle_mask[rr, cc] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_stack_from_path(decreasing_model, increasing_model, middle_model, stack_dir, metric_fn=lambda *x: 0):\n",
    "    # Load the stack\n",
    "    stack = preprocess_stack(load_stack(stack_dir))\n",
    "    # Set output directory\n",
    "    output_path = os.path.basename(stack_dir)\n",
    "    focals = ['F-75', 'F-60', 'F-45', 'F-30', 'F-15',\n",
    "              'F0', 'F15', 'F30', 'F45', 'F60', 'F75']\n",
    "    scores = []\n",
    "    # Randomly switch between in-between generation and extrapolation\n",
    "    if random.random() < 0.5:\n",
    "        # Pick random plane to remove (except for the ends)\n",
    "        idxs_to_remove = random.choices(range(1,len(focals) - 1), k=2)\n",
    "        focals_to_remove = [focals[idx] for idx in idxs_to_remove]\n",
    "        # Generate the missing ones\n",
    "        first_missing = predict_with_model(\n",
    "            middle_model, stack[idxs_to_remove[0] + 1], stack[idxs_to_remove[0] - 1]\n",
    "        ) * circle_mask\n",
    "        score = metric_fn(first_missing.unsqueeze(0).unsqueeze(0).float(), torch.tensor(stack[idxs_to_remove[0]]).unsqueeze(0).unsqueeze(0).float()) \n",
    "        first_missing[1:20, 1:20] = 1\n",
    "        second_missing = predict_with_model(\n",
    "            middle_model, stack[idxs_to_remove[1] + 1], stack[idxs_to_remove[1] - 1]\n",
    "        ) * circle_mask\n",
    "        score += metric_fn(second_missing.unsqueeze(0).unsqueeze(0).float(), torch.tensor(stack[idxs_to_remove[1]]).unsqueeze(0).unsqueeze(0).float()) \n",
    "        score /= 2\n",
    "        second_missing[1:20, 1:20] = 1\n",
    "        # Generate the missing planes\n",
    "        io.imsave(os.path.join(output_path, f'{focals_to_remove[0]}.jpg'), first_missing.numpy())\n",
    "        io.imsave(os.path.join(output_path, f'{focals_to_remove[1]}.jpg'), second_missing.numpy())\n",
    "    else:\n",
    "        # Generate missing planes and mark them as fake with a lil square in the top corner\n",
    "        focals = ['F-75', 'F-60', 'F-45', 'F-30', 'F-15',\n",
    "              'F0', 'F15', 'F30', 'F45', 'F60', 'F75']\n",
    "        plane_plus_60 = predict_with_model(\n",
    "            increasing_model, stack[8], stack[7]\n",
    "        ) * circle_mask\n",
    "        plane_plus_75 = predict_with_model(\n",
    "            increasing_model, plane_plus_60, stack[8]\n",
    "        ) * circle_mask\n",
    "        scores.append(metric_fn(plane_plus_60.unsqueeze(0).unsqueeze(0).float(), torch.tensor(stack[9]).unsqueeze(0).unsqueeze(0).float()))\n",
    "        scores.append(metric_fn(plane_plus_75.unsqueeze(0).unsqueeze(0).float(), torch.tensor(stack[10]).unsqueeze(0).unsqueeze(0).float()))\n",
    "        plane_plus_60[1:20, 1:20] = 1\n",
    "        plane_plus_75[1:20, 1:20] = 1\n",
    "        plane_minus_60 = predict_with_model(\n",
    "            decreasing_model, stack[2], stack[3]\n",
    "        )\n",
    "        plane_minus_75 = predict_with_model(\n",
    "            decreasing_model, plane_minus_60, stack[2]\n",
    "        )\n",
    "        scores.append(metric_fn(plane_plus_60.unsqueeze(0).unsqueeze(0).float(), torch.tensor(stack[9]).unsqueeze(0).unsqueeze(0).float()))\n",
    "        scores.append(metric_fn(plane_plus_75.unsqueeze(0).unsqueeze(0).float(), torch.tensor(stack[10]).unsqueeze(0).unsqueeze(0).float()))\n",
    "        plane_minus_60[1:20, 1:20] = 1\n",
    "        plane_minus_75[1:20, 1:20] = 1\n",
    "        # Save the new stack\n",
    "        io.imsave(os.path.join(output_path, 'F60.jpg'), plane_plus_60.numpy())\n",
    "        io.imsave(os.path.join(output_path, 'F75.jpg'), plane_plus_75.numpy())\n",
    "        io.imsave(os.path.join(output_path, 'F-60.jpg'),\n",
    "                  plane_minus_60.numpy())\n",
    "        io.imsave(os.path.join(output_path, 'F-75.jpg'),\n",
    "                  plane_minus_75.numpy())\n",
    "\n",
    "    # Save the new stack\n",
    "    for i, img in enumerate(stack):\n",
    "        io.imsave(os.path.join(output_path, f'{focals[i]}_gt.jpg'), img)\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "decreasing_model = torch.load('PLACEHOLDER')\n",
    "decreasing_model = decreasing_model.cuda(\n",
    ") if torch.cuda.is_available() else decreasing_model\n",
    "decreasing_model.eval()\n",
    "increasing_model = torch.load('PLACEHOLDER')\n",
    "increasing_model = increasing_model.cuda(\n",
    ") if torch.cuda.is_available() else increasing_model\n",
    "increasing_model.eval()\n",
    "middle_model = torch.load('PLACEHOLDER')\n",
    "middle_model = middle_model.cuda(\n",
    ") if torch.cuda.is_available() else middle_model\n",
    "middle_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Find all the stacks of interest\n",
    "stack_dirs = glob.glob('PLACEHOLDER')[20:220]\n",
    "model_paths = [\n",
    "    # TODO: ADD MODELS\n",
    "]\n",
    "for model_path in model_paths:\n",
    "    scores = []\n",
    "    model = torch.load(model_path)\n",
    "    model = model.cuda() if torch.cuda.is_available() else model\n",
    "    model.eval()\n",
    "    # Loop through and generate\n",
    "    for i, stack_dir in tqdm(enumerate(stack_dirs)):\n",
    "        # Load the stack\n",
    "        stack = preprocess_stack(load_stack(stack_dir))\n",
    "        # Generate them all\n",
    "        for idx in range(2, len(stack)):\n",
    "            plane = predict_with_model(\n",
    "                model, stack[idx - 1], stack[idx - 2]\n",
    "            ) * circle_mask\n",
    "            actual_plane = stack[idx]\n",
    "            scores.append(ssim(plane.unsqueeze(0).unsqueeze(0).cuda(), torch.tensor(stack[idx]).unsqueeze(0).unsqueeze(0).cuda()).item())\n",
    "    print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Find all the stacks of interest\n",
    "stack_dirs = glob.glob('PLACEHOLDER')[20:220]\n",
    "model_paths = [\n",
    "    # TODO: ADD MODELS\n",
    "]\n",
    "for model_path in model_paths:\n",
    "    scores = []\n",
    "    model = torch.load(model_path)\n",
    "    model = model.cuda() if torch.cuda.is_available() else model\n",
    "    model.eval()\n",
    "    # Loop through and generate\n",
    "    for i, stack_dir in tqdm(enumerate(stack_dirs)):\n",
    "        # Load the stack\n",
    "        stack = preprocess_stack(load_stack(stack_dir))\n",
    "        # Generate them all\n",
    "        for idx in range(2, len(stack)):\n",
    "            plane = predict_with_model(\n",
    "                model, stack[idx - 1], stack[idx - 2]\n",
    "            ) * circle_mask\n",
    "            actual_plane = stack[idx]\n",
    "            mse = torch.mean((plane - torch.tensor(stack[idx]))**2).item()\n",
    "            scores.append(10 * np.log10(1.0 / mse))\n",
    "    print(np.mean(scores), np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Find all the stacks of interest\n",
    "stack_dirs = glob.glob('PLACEHOLDER')[20:4020]\n",
    "scores = []\n",
    "model_paths = [\n",
    "    # TODO: ADD MODELS\n",
    "]\n",
    "k = 0\n",
    "for j, model_path in enumerate(model_paths):\n",
    "    os.mkdir(str(j))\n",
    "    os.mkdir(os.path.join(str(j), 'real'))\n",
    "    os.mkdir(os.path.join(str(j), 'fake'))\n",
    "    model = torch.load(model_path)\n",
    "    model = model.cuda() if torch.cuda.is_available() else model\n",
    "    model.eval()\n",
    "    # Loop through and generate\n",
    "    for i, stack_dir in enumerate(stack_dirs):\n",
    "        k += 1\n",
    "        # Load the stack\n",
    "        stack = preprocess_stack(load_stack(stack_dir))\n",
    "        # Generate them all\n",
    "        for idx in range(2, len(stack)):\n",
    "            plane = predict_with_model(\n",
    "                model, stack[idx - 1], stack[idx - 2]\n",
    "            ) * circle_mask\n",
    "            actual_plane = stack[idx]\n",
    "            io.imsave(os.path.join(str(j), 'real', f'{k}.jpg'), plane.numpy())\n",
    "            io.imsave(os.path.join(str(j), 'fake', f'{k}.jpg'), stack[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qualitative Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLACEHOLDER (7 -> 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the stacks of interest\n",
    "stack_dirs = glob.glob('PLACEHOLDER')\n",
    "# Filter out the 11 FP ones\n",
    "stack_dirs = [d for d in stack_dirs if len(glob.glob(f'{d}/*')) == 7]\n",
    "os.mkdir('PLACEHOLDER')\n",
    "# Loop through and generate\n",
    "for i, stack_dir in enumerate(random.sample(stack_dirs, 10)):\n",
    "    # Load the stack\n",
    "    stack = preprocess_stack(load_stack(stack_dir))\n",
    "    print(len(stack))\n",
    "    plane_plus_60 = predict_with_model(\n",
    "            increasing_model, stack[6], stack[5]\n",
    "        ) * circle_mask\n",
    "    plane_plus_75 = predict_with_model(\n",
    "            increasing_model, plane_plus_60, stack[6]\n",
    "        ) * circle_mask\n",
    "    plane_plus_60[1:20, 1:20] = 1\n",
    "    plane_plus_75[1:20, 1:20] = 1\n",
    "    plane_minus_60 = predict_with_model(\n",
    "        decreasing_model, stack[0], stack[1]\n",
    "    )\n",
    "    plane_minus_75 = predict_with_model(\n",
    "        decreasing_model, plane_minus_60, stack[0]\n",
    "    )\n",
    "    plane_minus_60[1:20, 1:20] = 1\n",
    "    plane_minus_75[1:20, 1:20] = 1\n",
    "    # Save the new stack\n",
    "    output_path = os.path.join('PLACEHOLDER', str(i))\n",
    "    os.mkdir(output_path)\n",
    "    io.imsave(os.path.join(output_path, 'F60.jpg'), plane_plus_60.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F75.jpg'), plane_plus_75.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F-60.jpg'),\n",
    "                plane_minus_60.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F-75.jpg'),\n",
    "                plane_minus_75.numpy())\n",
    "    # Add existing planes\n",
    "    focals = ['F-45', 'F-30', 'F-15',\n",
    "        'F0', 'F15', 'F30', 'F45']\n",
    "    for j, img in enumerate(stack):\n",
    "        io.imsave(os.path.join(output_path, f'{focals[j]}.jpg'), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLACEHOLDER - (shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the stacks of interest\n",
    "stack_dirs = glob.glob('PLACEHOLDER')\n",
    "os.mkdir('PLACEHOLDER')\n",
    "# Loop through and generate\n",
    "for i, stack_dir in enumerate(random.sample(stack_dirs, 10)):\n",
    "    # Load the stack\n",
    "    stack = preprocess_stack(load_stack(stack_dir))\n",
    "    plane_plus_90 = predict_with_model(\n",
    "            increasing_model, stack[10], stack[9]\n",
    "        ) * circle_mask\n",
    "    plane_plus_105 = predict_with_model(\n",
    "            increasing_model, plane_plus_90, stack[10]\n",
    "        ) * circle_mask\n",
    "    plane_plus_90[1:20, 1:20] = 1\n",
    "    plane_plus_105[1:20, 1:20] = 1\n",
    "    # Save the new stack\n",
    "    output_path = os.path.join('PLACEHOLDER', str(i))\n",
    "    os.mkdir(output_path)\n",
    "    io.imsave(os.path.join(output_path, 'F90.jpg'), plane_plus_90.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F105.jpg'), plane_plus_105.numpy())\n",
    "    # Add existing planes\n",
    "    focals = ['F-45', 'F-30', 'F-15',\n",
    "        'F0', 'F15', 'F30', 'F45', 'F60', 'F75']\n",
    "    for j, img in enumerate(stack[2:]):\n",
    "        io.imsave(os.path.join(output_path, f'{focals[j]}.jpg'), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLACEHOLDER (Random Removal of N planes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the stacks of interest\n",
    "stack_dirs = glob.glob('PLACEHOLDER')[20:]\n",
    "os.mkdir('PLACEHOLDER')\n",
    "os.mkdir('PLACEHOLDER')\n",
    "# Loop through and generate\n",
    "for i, stack_dir in enumerate(random.sample(stack_dirs, 10)):\n",
    "    focals = ['F-75', 'F-60', 'F-45', 'F-30', 'F-15',\n",
    "              'F0', 'F15', 'F30', 'F45', 'F60', 'F75']\n",
    "    # Load the stack\n",
    "    stack = preprocess_stack(load_stack(stack_dir))\n",
    "    print(len(stack))\n",
    "    # Pick random planes to remove\n",
    "    idxs_to_remove = random.sample(range(len(focals)), 4)\n",
    "    focals_to_remove = [focals[idx] for idx in idxs_to_remove]\n",
    "    for focal in focals_to_remove:\n",
    "        try:\n",
    "            focals.remove(focal)\n",
    "        except Exception:\n",
    "            print('ERROR REMOVING FOCAL')\n",
    "            continue\n",
    "    # Generate the missing ones\n",
    "    output_path = os.path.join('PLACEHOLDER-random', str(i))\n",
    "    output_path_orig = os.path.join('PLACEHOLDER-random-orig', str(i))\n",
    "    os.mkdir(output_path)\n",
    "    os.mkdir(output_path_orig)\n",
    "    for j, idx in enumerate(idxs_to_remove):\n",
    "        if idx == 0:\n",
    "            plane = predict_with_model(\n",
    "                decreasing_model, stack[idx + 1], stack[idx + 2]\n",
    "            ) * circle_mask\n",
    "        elif idx == len(stack) - 1:\n",
    "            plane = predict_with_model(\n",
    "                increasing_model, stack[idx - 1], stack[idx - 2]\n",
    "            ) * circle_mask\n",
    "        else:\n",
    "            plane = predict_with_model(\n",
    "                middle_model, stack[idx + 1], stack[idx - 1]\n",
    "            ) * circle_mask\n",
    "        plane[1:20, 1:20] = 1\n",
    "        io.imsave(os.path.join(output_path, f'{focals_to_remove[j]}.jpg'), plane.numpy())\n",
    "        io.imsave(os.path.join(output_path_orig, f'{focals_to_remove[j]}.jpg'), stack[idx])\n",
    "    for j, img in enumerate(stack):\n",
    "        if j not in idxs_to_remove:\n",
    "            io.imsave(os.path.join(output_path, f'{focals[j - len([x for x in idxs_to_remove if x < j])]}.jpg'), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLACEHOLDER (3 -> 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the stacks of interest\n",
    "stack_dirs = glob.glob('PLACEHOLDER')\n",
    "os.mkdir('PLACEHOLDER')\n",
    "# Loop through and generate\n",
    "for i, stack_dir in enumerate(random.sample(stack_dirs, 10)):\n",
    "    # Load the stack\n",
    "    stack = preprocess_stack(load_stack(stack_dir))\n",
    "    print(len(stack))\n",
    "    plane_plus_15 = predict_with_model(\n",
    "        middle_model, stack[2], stack[1]\n",
    "    ) * circle_mask\n",
    "    plane_minus_15 = predict_with_model(\n",
    "        middle_model, stack[1], stack[0]\n",
    "    ) * circle_mask\n",
    "    plane_plus_45 = predict_with_model(\n",
    "        increasing_model, stack[2], plane_plus_15\n",
    "    ) * circle_mask\n",
    "    plane_plus_60 = predict_with_model(\n",
    "        increasing_model, plane_plus_45, stack[2]\n",
    "    ) * circle_mask\n",
    "    plane_plus_75 = predict_with_model(\n",
    "        increasing_model, plane_plus_60, plane_plus_45,\n",
    "    ) * circle_mask\n",
    "    plane_minus_45 = predict_with_model(\n",
    "        decreasing_model, stack[0], plane_minus_15\n",
    "    ) * circle_mask\n",
    "    plane_minus_60 = predict_with_model(\n",
    "        decreasing_model, plane_minus_45, stack[0]\n",
    "    ) * circle_mask\n",
    "    plane_minus_75 = predict_with_model(\n",
    "        decreasing_model, plane_minus_60, plane_minus_45\n",
    "    ) * circle_mask\n",
    "    \n",
    "    plane_plus_15[1:20, 1:20] = 1\n",
    "    plane_plus_45[1:20, 1:20] = 1\n",
    "    plane_plus_60[1:20, 1:20] = 1\n",
    "    plane_plus_75[1:20, 1:20] = 1\n",
    "    plane_minus_15[1:20, 1:20] = 1\n",
    "    plane_minus_45[1:20, 1:20] = 1\n",
    "    plane_minus_60[1:20, 1:20] = 1\n",
    "    plane_minus_75[1:20, 1:20] = 1\n",
    "    # Save the new stack\n",
    "    output_path = os.path.join('PLACEHOLDER-insane', str(i))\n",
    "    os.mkdir(output_path)\n",
    "    io.imsave(os.path.join(output_path, 'F15.jpg'), plane_plus_15.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F45.jpg'), plane_plus_45.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F60.jpg'), plane_plus_60.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F75.jpg'), plane_plus_75.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F-15.jpg'),\n",
    "                plane_minus_15.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F-45.jpg'),\n",
    "                plane_minus_45.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F-60.jpg'),\n",
    "                plane_minus_60.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F-75.jpg'),\n",
    "                plane_minus_75.numpy())\n",
    "    # Add existing planes\n",
    "    focals = ['F-30', 'F0', 'F30']\n",
    "    for j, img in enumerate(stack):\n",
    "        io.imsave(os.path.join(output_path, f'{focals[j]}.jpg'), img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLACEHOLDER (Fully generated for blast grading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the stacks of interest\n",
    "stack_dirs = glob.glob('PLACEHOLDER')[20:]\n",
    "os.mkdir('PLACEHOLDER')\n",
    "# Loop through and generate\n",
    "for i, stack_dir in enumerate(random.sample(stack_dirs, 50)):\n",
    "    focals = ['F-75', 'F-60', 'F-45', 'F-30', 'F-15',\n",
    "              'F0', 'F15', 'F30', 'F45', 'F60', 'F75']\n",
    "    # Load the stack\n",
    "    stack = preprocess_stack(load_stack(stack_dir))\n",
    "    print(len(stack))\n",
    "    # Generate them all\n",
    "    output_path = os.path.join('PLACEHOLDER', f'{i}_fake')\n",
    "    os.mkdir(output_path)\n",
    "    for idx in range(len(stack)):\n",
    "        if idx == 0:\n",
    "            plane = predict_with_model(\n",
    "                decreasing_model, stack[idx + 1], stack[idx + 2]\n",
    "            ) * circle_mask\n",
    "        elif idx == len(stack) - 1:\n",
    "            plane = predict_with_model(\n",
    "                increasing_model, stack[idx - 1], stack[idx - 2]\n",
    "            ) * circle_mask\n",
    "        else:\n",
    "            plane = predict_with_model(\n",
    "                middle_model, stack[idx + 1], stack[idx - 1]\n",
    "            ) * circle_mask\n",
    "        io.imsave(os.path.join(output_path, f'{focals[idx]}.jpg'), plane.numpy())\n",
    "    # Generate the GT\n",
    "    output_path = os.path.join('PLACEHOLDER', f'{i}')\n",
    "    os.mkdir(output_path)\n",
    "    for idx in range(len(stack)):\n",
    "        io.imsave(os.path.join(output_path, f'{focals[idx]}.jpg'), stack[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_increasing_model = torch.load('PLACEHOLDER')\n",
    "alternative_increasing_model = alternative_increasing_model.cuda(\n",
    ") if torch.cuda.is_available() else alternative_increasing_model\n",
    "alternative_increasing_model.eval()\n",
    "\n",
    "focals = ['F-75', 'F-60', 'F-45', 'F-30', 'F-15',\n",
    "            'F0', 'F15', 'F30', 'F45', 'F60', 'F75']\n",
    "\n",
    "# Find all the stacks of interest\n",
    "stack_dirs = glob.glob('PLACEHOLDER')[20:]\n",
    "os.mkdir('ssim-comp')\n",
    "# Loop through and generate\n",
    "for i, stack_dir in enumerate(random.sample(stack_dirs, 30)):\n",
    "    # Load the stack\n",
    "    stack = preprocess_stack(load_stack(stack_dir))\n",
    "    idx = random.randint(2, len(stack) - 1)\n",
    "    plane = predict_with_model(\n",
    "            increasing_model, stack[idx - 1], stack[idx - 2]\n",
    "        ) * circle_mask\n",
    "    plane_alt = predict_with_model(\n",
    "            alternative_increasing_model, stack[idx - 1], stack[idx - 2]\n",
    "        ) * circle_mask\n",
    "    plane[1:20, 1:20] = 1\n",
    "    plane_alt[1:20, 1:20] = 1\n",
    "    # Save the new stack\n",
    "    output_path = os.path.join('ssim-comp', str(i))\n",
    "    os.mkdir(output_path)\n",
    "    io.imsave(os.path.join(output_path, f'F{random.randint(100, 10000)}.jpg'), plane.numpy())\n",
    "    io.imsave(os.path.join(output_path, f'F{random.randint(100, 10000)}.jpeg'), plane_alt.numpy())\n",
    "    io.imsave(os.path.join(output_path, f'{focals[idx]}.jpg'), stack[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_middle_focal(upper, lower):\n",
    "    image = np.zeros((400, 400, 2))\n",
    "    image[:, :, 0] = upper\n",
    "    image[:, :, 1] = lower\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1).unsqueeze(0).float()\n",
    "    image = image.cuda() if torch.cuda.is_available() else image\n",
    "    pred = middle_model.forward(image)\n",
    "    pred = pred.squeeze().detach().cpu() * circle_mask\n",
    "    return pred\n",
    "\n",
    "\n",
    "def generate_interpolated_stack(stack):\n",
    "    new_stack = []\n",
    "    for i in range(len(stack) - 1):\n",
    "        new_stack.append(stack[i])\n",
    "        new_stack.append(predict_middle_focal(stack[i], stack[i + 1]))\n",
    "    new_stack.append(stack[len(stack) - 1])\n",
    "    return new_stack\n",
    "\n",
    "stack_dirs = glob.glob('PLACEHOLDER')[20:]\n",
    "os.mkdir('superres')\n",
    "\n",
    "focals = ['F-75', 'F-67', 'F-60', 'F-53', 'F-45', 'F-37', 'F-30', 'F-23', 'F-15', 'F-7',\n",
    "            'F0', 'F7', 'F15', 'F23', 'F30', 'F37', 'F45', 'F53', 'F60', 'F67', 'F75']\n",
    "\n",
    "# Loop through and generate\n",
    "for i, stack_dir in enumerate(random.sample(stack_dirs, 10)):\n",
    "    output_path = f'superres/{i}'\n",
    "    os.mkdir(output_path)\n",
    "    # Load the stack\n",
    "    stack = preprocess_stack(load_stack(stack_dir))\n",
    "    # Generate\n",
    "    stack = generate_interpolated_stack(stack)\n",
    "    # Save\n",
    "    for j, plane in enumerate(stack):\n",
    "        io.imsave(os.path.join(output_path, f'{focals[j]}.jpg'), plane)\n",
    "    # fig, ax = plt.subplots(3, int(np.ceil(len(stack) / 3)))\n",
    "    # fig.set_size_inches(21, 9)\n",
    "    # for i in range(len(stack)):\n",
    "    #     ax[int(np.floor(i / 7)), i % 7].imshow(stack[i])\n",
    "    # fig.savefig('test.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the stacks of interest\n",
    "stack_dirs = glob.glob('PLACEHOLDER')\n",
    "os.mkdir('PLACEHOLDER')\n",
    "os.mkdir('PLACEHOLDER')\n",
    "\n",
    "with_segs = [\n",
    "    # TODO: ADD PATHS\n",
    "]\n",
    "\n",
    "stack_dirs = [x for x in stack_dirs if os.path.basename(x) in with_segs]\n",
    "\n",
    "# Loop through and generate\n",
    "for i, stack_dir in enumerate(stack_dirs):\n",
    "    # Load the stack\n",
    "    stack = preprocess_stack(load_stack(stack_dir))\n",
    "    plane_plus_90 = predict_with_model(\n",
    "            increasing_model, stack[10], stack[9]\n",
    "        ) * circle_mask\n",
    "    plane_plus_105 = predict_with_model(\n",
    "            increasing_model, plane_plus_90, stack[10]\n",
    "        ) * circle_mask\n",
    "    plane_plus_90[1:20, 1:20] = 1\n",
    "    plane_plus_105[1:20, 1:20] = 1\n",
    "    # Save the new stack\n",
    "    output_path = os.path.join('PLACEHOLDER', os.path.basename(stack_dir))\n",
    "    output_path_orig = os.path.join('PLACEHOLDER', os.path.basename(stack_dir))\n",
    "    os.mkdir(output_path)\n",
    "    os.mkdir(output_path_orig)\n",
    "    io.imsave(os.path.join(output_path, 'F90.jpg'), plane_plus_90.numpy())\n",
    "    io.imsave(os.path.join(output_path, 'F105.jpg'), plane_plus_105.numpy())\n",
    "    # Add existing planes\n",
    "    focals = ['F-45', 'F-30', 'F-15',\n",
    "        'F0', 'F15', 'F30', 'F45', 'F60', 'F75']\n",
    "    for j, img in enumerate(stack[2:]):\n",
    "        io.imsave(os.path.join(output_path, f'{focals[j]}.jpg'), img)\n",
    "    # Add existing planes\n",
    "    focals = ['F-75', 'F-60', 'F-45', 'F-30', 'F-15',\n",
    "        'F0', 'F15', 'F30', 'F45', 'F60', 'F75']\n",
    "    for j, img in enumerate(stack):\n",
    "        io.imsave(os.path.join(output_path_orig, f'{focals[j]}.jpg'), img)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
